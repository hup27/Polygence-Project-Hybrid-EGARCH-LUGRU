{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eaf241",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GARCH'''\n",
    "from typing import Sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.optimize\n",
    "import scipy.stats\n",
    "\n",
    "def b1_square(x_view: np.ndarray) -> np.ndarray:\n",
    "    return np.square(x_view[-1, :])\n",
    "\n",
    "\n",
    "def b1(sigma_view: np.ndarray) -> np.ndarray:\n",
    "    return sigma_view[-1, :]\n",
    "\n",
    "\n",
    "def loglik(ret: np.ndarray, params: Sequence, vola_ret_features=b1_square, vola_sigma_features=b1):\n",
    "    \"\"\"Calculate the likelihood of a process path\"\"\"\n",
    "    ret = ret.reshape((-1, 1))\n",
    "    gamma_0, gamma_1, lambda_1 = params\n",
    "    sigma_squared = np.repeat(gamma_0, len(ret)).reshape(ret.shape)\n",
    "    sigma_squared[0, 0] = gamma_0\n",
    "\n",
    "    for s in range(1, len(ret)):\n",
    "        sigma_squared[s, :] = (\n",
    "            gamma_0\n",
    "            + np.dot(gamma_1, vola_ret_features(ret[0:s, :]))\n",
    "            + np.dot(lambda_1, vola_sigma_features(sigma_squared[0:s, :]))\n",
    "        )\n",
    "\n",
    "    return np.sum(scipy.stats.norm.logpdf(ret[1:, 0], loc=0.0, scale=np.sqrt(sigma_squared[1:, 0])))\n",
    "\n",
    "\n",
    "def mle(ret: np.ndarray, start_params: Sequence):\n",
    "    \"\"\"Maximum-likelihood estimator\"\"\"\n",
    "\n",
    "    def error_fuc(theta):\n",
    "        return -loglik(ret, theta)\n",
    "\n",
    "    start_params = np.array(start_params)\n",
    "    result = scipy.optimize.minimize(\n",
    "        error_fuc,\n",
    "        start_params,\n",
    "        method='L-BFGS-B',\n",
    "        bounds=[(1e-8, 1.0), (1e-8, 1.0), (1e-8, 1.0)],\n",
    "        options={'maxiter': 250, 'disp': False},\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "def path(\n",
    "    no_paths: int, t: int, params: Sequence, vola_ret_features=b1_square, vola_sigma_features=b1\n",
    "):\n",
    "    \"\"\"Simulate process paths\"\"\"\n",
    "    assert no_paths > 0 and t > 0\n",
    "    assert len(params) == 3\n",
    "\n",
    "    ret = np.random.randn(t, no_paths)\n",
    "    gamma_0, gamma_1, lambda_1 = params\n",
    "    gamma_0 = np.repeat(gamma_0, no_paths).reshape((1, no_paths))\n",
    "    sigma_squared = np.zeros((t, no_paths))\n",
    "    sigma_squared[0, :] = gamma_0\n",
    "    ret[0, :] = 0.0\n",
    "\n",
    "    for s in range(1, t):\n",
    "        sigma_squared[s, :] = (\n",
    "            gamma_0\n",
    "            + np.dot(gamma_1, vola_ret_features(ret[0:s, :]))\n",
    "            + np.dot(lambda_1, vola_sigma_features(sigma_squared[0:s, :]))\n",
    "        )\n",
    "        ret[s, :] = ret[s, :] * np.sqrt(sigma_squared[s, :])\n",
    "\n",
    "    return ret, np.sqrt(sigma_squared)\n",
    "\n",
    "\n",
    "def noise_from_path(\n",
    "    ret: np.ndarray, params: Sequence, vola_ret_features=b1_square, vola_sigma_features=b1\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Extract the noise process path from a GARCH path given a parameter set\"\"\"\n",
    "    ret = ret.reshape((-1, 1))\n",
    "    gamma_0, gamma_1, lambda_1 = params\n",
    "    sigma_squared = np.repeat(gamma_0, len(ret)).reshape(ret.shape)\n",
    "    sigma_squared[0, 0] = gamma_0\n",
    "    noise = np.zeros(ret.shape)\n",
    "\n",
    "    noise[0, :] = ret[0, :] / np.sqrt(sigma_squared[0, :])\n",
    "    for s in range(1, len(ret)):\n",
    "        sigma_squared[s, :] = (\n",
    "            gamma_0\n",
    "            + np.dot(gamma_1, vola_ret_features(ret[0:s, :]))\n",
    "            + np.dot(lambda_1, vola_sigma_features(sigma_squared[0:s, :]))\n",
    "        )\n",
    "        noise[s, :] = ret[s, :] / np.sqrt(sigma_squared[s, :])\n",
    "    return noise\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    plt.style.use('ggplot')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eb2549",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2867626853.py, line 52)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mbounds = [\u001b[39m\n             ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "'''EGARCH'''\n",
    "from typing import Sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.optimize\n",
    "import scipy.stats\n",
    "\n",
    "def b1_square(x_view: np.ndarray) -> np.ndarray:\n",
    "    return np.square(x_view[-1, :])\n",
    "\n",
    "\n",
    "def b1(sigma_view: np.ndarray) -> np.ndarray:\n",
    "    return sigma_view[-1, :]\n",
    "\n",
    "E_ABS_Z = np.sqrt(2 / np.pi)\n",
    "\n",
    "def loglik(ret: np.ndarray, params: Sequence, vola_ret_features=b1_square, vola_sigma_features=b1):\n",
    "    \"\"\"Calculate the likelihood of a process path\"\"\"\n",
    "    ret = ret.reshape((-1, 1))\n",
    "    omega, alpha, gamma, beta = params\n",
    "    log_sigma_squared = np.zeros_like(ret)\n",
    "    log_sigma_squared[0, 0] = omega\n",
    "\n",
    "    for s in range(1, len(ret)):\n",
    "        sigma_prev = np.sqrt(np.exp(log_sigma_squared[s - 1, 0]))\n",
    "        z_prev = ret[s - 1, 0] / sigma_prev\n",
    "        log_sigma_squared[s, :] = (\n",
    "            omega\n",
    "            + beta * np.log(sigma_prev**2)\n",
    "            + alpha * (np.abs(z_prev) - E_ABS_Z)\n",
    "            + gamma * z_prev\n",
    "        )\n",
    "\n",
    "    sigma_squared = np.exp(log_sigma_squared)\n",
    "\n",
    "    return np.sum(scipy.stats.norm.logpdf(ret[1:, 0], loc=0.0, scale=np.sqrt(sigma_squared[1:, 0])))\n",
    "\n",
    "\n",
    "def mle(ret: np.ndarray, start_params: Sequence):\n",
    "    \"\"\"Maximum-likelihood estimator\"\"\"\n",
    "\n",
    "    def error_fuc(theta):\n",
    "        return -loglik(ret, theta)\n",
    "\n",
    "    start_params = np.array(start_params)\n",
    "    result = scipy.optimize.minimize(\n",
    "        error_fuc,\n",
    "        start_params,\n",
    "        method='L-BFGS-B',\n",
    "        bounds = [\n",
    "        (None, None),   # omega\n",
    "        (0.0, 1.0),     # alpha\n",
    "        (None, None),   # gamma\n",
    "        (0.0, 1.0)],    # beta]\n",
    "        options={'maxiter': 250, 'disp': False},\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "def path(\n",
    "    no_paths: int, t: int, params: Sequence, vola_ret_features=b1_square, vola_sigma_features=b1\n",
    "):\n",
    "    \"\"\"Simulate process paths\"\"\"\n",
    "    assert no_paths > 0 and t > 0\n",
    "    assert len(params) == 3\n",
    "\n",
    "    ret = np.random.randn(t, no_paths)\n",
    "    omega, alpha, gamma, beta = params\n",
    "    log_sigma_squared = np.zeros((t, no_paths))\n",
    "    log_sigma_squared[0, :] = omega\n",
    "    z = np.random.randn(t, no_paths)\n",
    "    ret[0, :] = 0.0\n",
    "\n",
    "    for s in range(1, t):\n",
    "        sigma_prev = np.sqrt(np.exp(log_sigma_squared[s - 1, :]))\n",
    "        z_prev = ret[s - 1, :] / sigma_prev\n",
    "        log_sigma_squared[s, :] = (\n",
    "            omega\n",
    "            + beta * np.log(sigma_prev**2)\n",
    "            + alpha * (np.abs(z_prev) - E_ABS_Z)\n",
    "            + gamma * z_prev\n",
    "        )\n",
    "        ret[s, :] = z[s, :] * np.sqrt(log_sigma_squared[s, :])\n",
    "\n",
    "    return ret, np.sqrt(log_sigma_squared)\n",
    "\n",
    "\n",
    "def noise_from_path(ret: np.ndarray, params: Sequence):\n",
    "    \"\"\"\n",
    "    Extract standardized residuals z_t from EGARCH path\n",
    "    \"\"\"\n",
    "\n",
    "    ret = ret.reshape((-1, 1))\n",
    "    omega, alpha, gamma, beta = params\n",
    "\n",
    "    log_sigma_sq = np.zeros_like(ret)\n",
    "    log_sigma_sq[0, 0] = omega\n",
    "    z = np.zeros_like(ret)\n",
    "\n",
    "    for t in range(1, len(ret)):\n",
    "        sigma_prev = np.sqrt(np.exp(log_sigma_sq[t - 1, 0]))\n",
    "        z_prev = ret[t - 1, 0] / sigma_prev\n",
    "\n",
    "        log_sigma_sq[t, 0] = (\n",
    "            omega\n",
    "            + beta * log_sigma_sq[t - 1, 0]\n",
    "            + alpha * (np.abs(z_prev) - E_ABS_Z)\n",
    "            + gamma * z_prev\n",
    "        )\n",
    "\n",
    "        z[t, 0] = ret[t, 0] / np.sqrt(np.exp(log_sigma_sq[t, 0]))\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    plt.style.use('ggplot')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b93655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"GRU\"\"\"\n",
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GRUCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim) -> None:\n",
    "        super(GRUCell, self).__init__()\n",
    "        self.input_dim, self.hidden_dim = input_dim, hidden_dim\n",
    "        self.relevance_whh, self.relevance_wxh, self.relevance_b = self.create_gate_parameters()\n",
    "        self.update_whh, self.update_wxh, self.update_b = self.create_gate_parameters()\n",
    "        self.candidate_whh, self.candidate_wxh, self.candidate_b = self.create_gate_parameters()\n",
    "\n",
    "    def create_gate_parameters(self):\n",
    "        input_weights = nn.Parameter(torch.zeros(self.input_dim, self.hidden_dim))\n",
    "        hidden_weights = nn.Parameter(torch.zeros(self.hidden_dim, self.hidden_dim))\n",
    "        nn.init.xavier_uniform_(input_weights)\n",
    "        nn.init.xavier_uniform_(hidden_weights)\n",
    "        bias = nn.Parameter(torch.zeros(self.hidden_dim))\n",
    "        return hidden_weights, input_weights, bias\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        output_hiddens = []\n",
    "        for i in range(x.shape[1]):\n",
    "            relevance_gate = F.sigmoid((h @ self.relevance_whh) + (x[:, i] @ self.relevance_wxh) + self.relevance_b)\n",
    "            update_gate = F.sigmoid((h @ self.update_whh) + (x[:, i] @ self.update_wxh) + self.update_b)\n",
    "            candidate_hidden = F.tanh(((relevance_gate * h) @ self.candidate_whh) + (x[:, i] @ self.candidate_wxh) + self.candidate_b)\n",
    "            h = (update_gate * candidate_hidden) + ((1 - update_gate) * h)\n",
    "            output_hiddens.append(h.unsqueeze(1))\n",
    "        return torch.concat(output_hiddens, dim=1)\n",
    "\n",
    "\n",
    "class MultiLayerGRU(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        super(MultiLayerGRU, self).__init__()\n",
    "        self.input_dim, self.hidden_dim, self.num_layers = input_dim, hidden_dim, num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(GRUCell(input_dim, hidden_dim))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(GRUCell(hidden_dim, hidden_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(hidden_dim, input_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight.data)\n",
    "        self.linear.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        output_hidden = self.layers[0](x, h[0])\n",
    "        new_hidden = [output_hidden[:, -1].unsqueeze(0)]\n",
    "        for i in range(1, self.num_layers):\n",
    "            output_hidden = self.layers[i](self.dropout(output_hidden), h[i])\n",
    "            new_hidden.append(output_hidden[:, -1].unsqueeze(0))\n",
    "        return self.linear(self.dropout(output_hidden)), torch.concat(new_hidden, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6082f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"LIGRU\"\n",
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GRUCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim) -> None:\n",
    "        super(GRUCell, self).__init__()\n",
    "        self.input_dim, self.hidden_dim = input_dim, hidden_dim\n",
    "        self.update_whh, self.update_wxh, self.update_b = self.create_gate_parameters()\n",
    "        self.candidate_whh, self.candidate_wxh, self.candidate_b = self.create_gate_parameters()\n",
    "\n",
    "    def create_gate_parameters(self):\n",
    "        input_weights = nn.Parameter(torch.zeros(self.input_dim, self.hidden_dim))\n",
    "        hidden_weights = nn.Parameter(torch.zeros(self.hidden_dim, self.hidden_dim))\n",
    "        nn.init.xavier_uniform_(input_weights)\n",
    "        nn.init.xavier_uniform_(hidden_weights)\n",
    "        bias = nn.Parameter(torch.zeros(self.hidden_dim))\n",
    "        return hidden_weights, input_weights, bias\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        output_hiddens = []\n",
    "        for i in range(x.shape[1]):\n",
    "            update_gate = F.sigmoid((h @ self.update_whh) + (x[:, i] @ self.update_wxh) + self.update_b)\n",
    "            candidate_hidden = F.relu((h @ self.candidate_whh) + ((x[:, i] @ self.candidate_wxh) + self.candidate_b))\n",
    "            h = (update_gate * candidate_hidden) + ((1 - update_gate) * h)\n",
    "            output_hiddens.append(h.unsqueeze(1))\n",
    "        return torch.concat(output_hiddens, dim=1)\n",
    "\n",
    "\n",
    "class MultiLayerGRU(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        super(MultiLayerGRU, self).__init__()\n",
    "        self.input_dim, self.hidden_dim, self.num_layers = input_dim, hidden_dim, num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(GRUCell(input_dim, hidden_dim))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(GRUCell(hidden_dim, hidden_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(hidden_dim, input_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight.data)\n",
    "        self.linear.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        output_hidden = self.layers[0](x, h[0])\n",
    "        new_hidden = [output_hidden[:, -1].unsqueeze(0)]\n",
    "        for i in range(1, self.num_layers):\n",
    "            output_hidden = self.layers[i](self.dropout(output_hidden), h[i])\n",
    "            new_hidden.append(output_hidden[:, -1].unsqueeze(0))\n",
    "        return self.linear(self.dropout(output_hidden)), torch.concat(new_hidden, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5412467",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of bounds is not compatible with the length of `x0`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\scipy\\optimize\\_minimize.py:1046\u001b[39m, in \u001b[36m_validate_bounds\u001b[39m\u001b[34m(bounds, x0, meth)\u001b[39m\n\u001b[32m   1045\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1046\u001b[39m     bounds.lb = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbroadcast_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1047\u001b[39m     bounds.ub = np.broadcast_to(bounds.ub, x0.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\lib\\_stride_tricks_impl.py:410\u001b[39m, in \u001b[36mbroadcast_to\u001b[39m\u001b[34m(array, shape, subok)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Broadcast an array to a new shape.\u001b[39;00m\n\u001b[32m    370\u001b[39m \n\u001b[32m    371\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    408\u001b[39m \u001b[33;03m       [1, 2, 3]])\u001b[39;00m\n\u001b[32m    409\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_broadcast_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreadonly\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\lib\\_stride_tricks_impl.py:349\u001b[39m, in \u001b[36m_broadcast_to\u001b[39m\u001b[34m(array, shape, subok, readonly)\u001b[39m\n\u001b[32m    348\u001b[39m extras = []\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m it = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnditer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmulti_index\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrefs_ok\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mzerosize_ok\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mextras\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mop_flags\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mreadonly\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitershape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m it:\n\u001b[32m    353\u001b[39m     \u001b[38;5;66;03m# never really has writebackifcopy semantics\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with remapped shapes [original->remapped]: (3,)  and requested shape (4,)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m returns = returns - returns.mean()\n\u001b[32m     16\u001b[39m start_params = [-\u001b[32m0.1\u001b[39m, \u001b[32m0.1\u001b[39m, \u001b[32m0.0\u001b[39m, \u001b[32m0.9\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m result = \u001b[43mmle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m params_hat = result.x\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEstimated parameters:\u001b[39m\u001b[33m\"\u001b[39m, params_hat)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mmle\u001b[39m\u001b[34m(ret, start_params)\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m -loglik(ret, theta)\n\u001b[32m     47\u001b[39m start_params = np.array(start_params)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m result = \u001b[43mscipy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m.\u001b[49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_fuc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mL-BFGS-B\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1e-8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1e-8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1e-8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmaxiter\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdisp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\scipy\\optimize\\_minimize.py:671\u001b[39m, in \u001b[36mminimize\u001b[39m\u001b[34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[39m\n\u001b[32m    668\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bounds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    669\u001b[39m     \u001b[38;5;66;03m# convert to new-style bounds so we only have to consider one case\u001b[39;00m\n\u001b[32m    670\u001b[39m     bounds = standardize_bounds(bounds, x0, \u001b[33m'\u001b[39m\u001b[33mnew\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m671\u001b[39m     bounds = \u001b[43m_validate_bounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    673\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m meth \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mtnc\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mslsqp\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33ml-bfgs-b\u001b[39m\u001b[33m\"\u001b[39m}:\n\u001b[32m    674\u001b[39m         \u001b[38;5;66;03m# These methods can't take the finite-difference derivatives they\u001b[39;00m\n\u001b[32m    675\u001b[39m         \u001b[38;5;66;03m# need when a variable is fixed by the bounds. To avoid this issue,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    680\u001b[39m \n\u001b[32m    681\u001b[39m         \u001b[38;5;66;03m# determine whether any variables are fixed\u001b[39;00m\n\u001b[32m    682\u001b[39m         i_fixed = (bounds.lb == bounds.ub)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\scipy\\optimize\\_minimize.py:1049\u001b[39m, in \u001b[36m_validate_bounds\u001b[39m\u001b[34m(bounds, x0, meth)\u001b[39m\n\u001b[32m   1047\u001b[39m     bounds.ub = np.broadcast_to(bounds.ub, x0.shape)\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m bounds\n",
      "\u001b[31mValueError\u001b[39m: The number of bounds is not compatible with the length of `x0`."
     ]
    }
   ],
   "source": [
    "\"Example Usage\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "dataset = pd.read_csv(\"airline-passengers.csv\")\n",
    "\n",
    "# parse time\n",
    "dataset[\"Month\"] = pd.to_datetime(dataset[\"Month\"])\n",
    "dataset = dataset.sort_values(\"Month\")\n",
    "\n",
    "values = dataset[\"Passengers\"].values.astype(\"float32\")\n",
    "log_values = np.log(values)\n",
    "\n",
    "returns = np.diff(log_values)\n",
    "returns = returns - returns.mean()\n",
    "start_params = [-0.1, 0.1, 0.0, 0.9]\n",
    "result = mle(returns, start_params)\n",
    "params_hat = result.x\n",
    "\n",
    "print(\"Estimated parameters:\", params_hat)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
